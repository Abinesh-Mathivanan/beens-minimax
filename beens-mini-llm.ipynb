{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":465119,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":375299,"modelId":396124},{"sourceId":467510,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":377161,"modelId":397749}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Beens-Minimax (Base - 103M Params)","metadata":{}},{"cell_type":"markdown","source":"## Necessary Imports","metadata":{}},{"cell_type":"code","source":"!pip install -q evaluate accelerate einops peft bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:45:44.696772Z","iopub.execute_input":"2025-07-11T06:45:44.697016Z","iopub.status.idle":"2025-07-11T06:46:58.791611Z","shell.execute_reply.started":"2025-07-11T06:45:44.696986Z","shell.execute_reply":"2025-07-11T06:46:58.790650Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport evaluate\nimport time\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, get_scheduler, DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\nfrom datasets import load_dataset, load_from_disk\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:47:05.242145Z","iopub.execute_input":"2025-07-11T06:47:05.242720Z","iopub.status.idle":"2025-07-11T06:47:31.904607Z","shell.execute_reply.started":"2025-07-11T06:47:05.242653Z","shell.execute_reply":"2025-07-11T06:47:31.904019Z"}},"outputs":[{"name":"stderr","text":"2025-07-11 06:47:16.247399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752216436.444950      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752216436.496434      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from peft import PeftModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:55:24.909020Z","iopub.execute_input":"2025-07-11T06:55:24.909339Z","iopub.status.idle":"2025-07-11T06:55:24.913188Z","shell.execute_reply.started":"2025-07-11T06:55:24.909319Z","shell.execute_reply":"2025-07-11T06:55:24.912479Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"#### you should update to version >= 4.0.0, will face error otherwise.","metadata":{}},{"cell_type":"code","source":"!pip install -U datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:47:46.847230Z","iopub.execute_input":"2025-07-11T06:47:46.847907Z","iopub.status.idle":"2025-07-11T06:47:50.999960Z","shell.execute_reply.started":"2025-07-11T06:47:46.847879Z","shell.execute_reply":"2025-07-11T06:47:50.999161Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nCollecting datasets\n  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.6.0\n    Uninstalling datasets-3.6.0:\n      Successfully uninstalled datasets-3.6.0\nSuccessfully installed datasets-4.0.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Beens-minimax configuration","metadata":{}},{"cell_type":"code","source":"class MiniMaxConfig:\n    def __init__(self, vocab_size):\n        self.vocab_size = vocab_size\n        self.hidden_size = 512\n        self.num_layers = 8\n        self.num_attention_heads = 8\n        self.head_dim = self.hidden_size // self.num_attention_heads\n        self.gqa_group_size = 2\n        self.num_key_value_heads = self.num_attention_heads // self.gqa_group_size\n        self.num_experts = 4\n        self.num_experts_per_tok = 2\n        self.ffn_hidden_dim = 2048\n        self.rope_base = 10_000\n        self.rope_dim_fraction = 0.5\n        self.softmax_attention_period = 4\n        self.deepnorm_alpha = (2 * self.num_layers) ** 0.25\n        self.rms_norm_eps = 1e-6\n        self.router_aux_loss_coef = 0.01\n        \n        self.tie_word_embeddings = True \n        self.model_type = \"minimax_custom\"\n\n    def to_dict(self):\n        return self.__dict__\n\n    def get(self, key, default=None):\n        return getattr(self, key, default)\n\n\nprint(\"Configuration defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:47:54.690883Z","iopub.execute_input":"2025-07-11T06:47:54.691192Z","iopub.status.idle":"2025-07-11T06:47:54.699457Z","shell.execute_reply.started":"2025-07-11T06:47:54.691164Z","shell.execute_reply":"2025-07-11T06:47:54.698652Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"KAGGLE_WORKING_PATH = \"/kaggle/working/\"\nDATA_PATH = os.path.join(KAGGLE_WORKING_PATH, \"processed_data\")\nSFT_LORA_CHECKPOINT_PATH = os.path.join(KAGGLE_WORKING_PATH, \"sft_lora_checkpoints\")\nBASE_MODEL_INPUT_PATH = \"/kaggle/input/beens-2/pytorch/default/1/\"\nTOKENIZER_INPUT_PATH = \"/kaggle/input/minimax-processed-data/processed_data/\"\n\nos.makedirs(SFT_LORA_CHECKPOINT_PATH, exist_ok=True)\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nprint(\"Paths defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:48:04.616474Z","iopub.execute_input":"2025-07-11T06:48:04.617204Z","iopub.status.idle":"2025-07-11T06:48:04.621523Z","shell.execute_reply.started":"2025-07-11T06:48:04.617177Z","shell.execute_reply":"2025-07-11T06:48:04.620898Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Data Pre-processing","metadata":{}},{"cell_type":"code","source":"def prepare_and_save_data():\n    BLOCK_SIZE = 256\n    DATASET_NAME = \"wikitext\"\n    DATASET_CONFIG = \"wikitext-103-v1\"\n    TOKENIZER_NAME = \"gpt2\"\n    \n    if os.path.exists(os.path.join(DATA_PATH, \"train\")):\n        print(\"Processed data already found in /kaggle/working/. Skipping preparation.\")\n        return\n\n\n    \n    print(\"--- Starting Data Preparation ---\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n    if tokenizer.pad_token is None: \n        tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.model_max_length = 1024\n\n    print(f\"Downloading '{DATASET_NAME}' dataset from Hugging Face Hub...\")\n    raw_datasets = load_dataset(DATASET_NAME, DATASET_CONFIG)\n    print(\"Dataset downloaded.\")\n\n    \n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], add_special_tokens=False, truncation=True, max_length=tokenizer.model_max_length)\n\n    print(\"Tokenizing dataset...\")\n    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, num_proc=2, remove_columns=[\"text\"])\n\n    \n\n    def group_texts(examples):\n        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n        result = {k: [t[i : i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)] for k, t in concatenated_examples.items()}\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n    print(\"Grouping texts into blocks...\")\n\n    \n    lm_datasets = tokenized_datasets.map(group_texts, batched=True, batch_size=1000, num_proc=2)\n    print(f\"\\nSaving processed datasets to '{DATA_PATH}'...\")\n\n    \n    lm_datasets[\"train\"].save_to_disk(os.path.join(DATA_PATH, \"train\"))\n    lm_datasets[\"validation\"].save_to_disk(os.path.join(DATA_PATH, \"validation\"))\n    tokenizer.save_pretrained(DATA_PATH)\n    print(\"Data preparation and saving complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prepare_and_save_data()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RMS and RoPE Embeddings","metadata":{}},{"cell_type":"code","source":"def rotate_half(x): \n    x1, x2 = x.chunk(2, -1); return torch.cat((-x2, x1), -1)\n\ndef apply_rotary_pos_emb(q, k, cos, sin):\n    cos = cos.unsqueeze(0).unsqueeze(1)\n    sin = sin.unsqueeze(0).unsqueeze(1)\n    \n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim, eps): \n        super().__init__(); \n        self.eps, self.weight = eps, nn.Parameter(torch.ones(dim))\n        \n    def forward(self, x): \n        return x * torch.rsqrt(x.pow(2).mean(-1, True) + self.eps) * self.weight\n\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim, base=10000):\n        super().__init__(); \n        inv_freq = 1.0 / (base**(torch.arange(0, dim, 2).float() / dim)) \n        self.register_buffer(\"inv_freq\", inv_freq)\n        self.cos_cached, self.sin_cached = None, None\n        \n    def _update_cache(self, x, seq_len):\n        if self.cos_cached is not None and seq_len <= self.cos_cached.shape[0]: \n            return\n            \n        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq); \n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        emb = torch.cat((freqs, freqs), -1)\n        self.cos_cached, self.sin_cached = emb.cos(), emb.sin()\n\n    \n    def forward(self, x, seq_len): \n        self._update_cache(x, seq_len)\n        return self.cos_cached[:seq_len].to(x.dtype), self.sin_cached[:seq_len].to(x.dtype)\n\n\nprint(\"RMS and ROPE Utilities defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:48:18.322366Z","iopub.execute_input":"2025-07-11T06:48:18.323065Z","iopub.status.idle":"2025-07-11T06:48:18.337079Z","shell.execute_reply.started":"2025-07-11T06:48:18.323024Z","shell.execute_reply":"2025-07-11T06:48:18.336229Z"}},"outputs":[{"name":"stdout","text":"RMS and ROPE Utilities defined\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Minimax Attention Layers","metadata":{}},{"cell_type":"code","source":"class SoftmaxAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__(); self.config = config; self.rope_dim = int(config.head_dim * config.rope_dim_fraction)\n        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * config.head_dim, bias=False)\n        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * config.head_dim, bias=False)\n        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * config.head_dim, bias=False)\n        self.o_proj = nn.Linear(config.num_attention_heads * config.head_dim, config.hidden_size, bias=False)\n        self.rotary_emb = RotaryEmbedding(self.rope_dim)\n        \n    def forward(self, x, mask, output_attentions=False):\n        bsz, seq_len, _ = x.shape\n        q_s = self.q_proj(x).view(bsz, seq_len, self.config.num_attention_heads, self.config.head_dim)\n        k_s = self.k_proj(x).view(bsz, seq_len, self.config.num_key_value_heads, self.config.head_dim)\n        v_s = self.v_proj(x).view(bsz, seq_len, self.config.num_key_value_heads, self.config.head_dim)\n        \n        q_s, k_s, v_s = q_s.transpose(1, 2), k_s.transpose(1, 2), v_s.transpose(1, 2)\n        \n        q_rot, q_pass = q_s[..., :self.rope_dim], q_s[..., self.rope_dim:]\n        k_rot, k_pass = k_s[..., :self.rope_dim], k_s[..., self.rope_dim:]\n        \n        cos, sin = self.rotary_emb(v_s, seq_len=seq_len)\n        q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, cos, sin)\n        \n        q_s = torch.cat((q_rot, q_pass), dim=-1)\n        k_s = torch.cat((k_rot, k_pass), dim=-1)\n        \n        k_s = k_s.repeat_interleave(self.config.gqa_group_size, dim=1)\n        v_s = v_s.repeat_interleave(self.config.gqa_group_size, dim=1)\n        \n        attn_weights = torch.matmul(q_s, k_s.transpose(2, 3)) / math.sqrt(self.config.head_dim)\n        if mask is not None:\n            attn_weights += mask\n        \n        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q_s.dtype)\n        attn_output = torch.matmul(attn_weights, v_s).transpose(1, 2).contiguous().view(bsz, seq_len, self.config.hidden_size)\n        \n        return self.o_proj(attn_output), attn_weights if output_attentions else None\n\n\n\nclass LightningAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.q_proj, self.k_proj, self.v_proj, self.g_proj, self.o_proj = (nn.Linear(config.hidden_size, config.hidden_size, bias=False) for _ in range(5))\n\n    \n    def forward(self, x, mask, output_attentions=False):\n        bsz, seqlen, _ = x.shape\n        \n        q = F.silu(self.q_proj(x)).view(bsz, seqlen, self.config.num_attention_heads, self.config.head_dim).transpose(1, 2)\n        k = F.silu(self.k_proj(x)).view(bsz, seqlen, self.config.num_attention_heads, self.config.head_dim).transpose(1, 2)\n        v = F.silu(self.v_proj(x)).view(bsz, seqlen, self.config.num_attention_heads, self.config.head_dim).transpose(1, 2)\n        g = F.sigmoid(self.g_proj(x)).view(bsz, seqlen, self.config.num_attention_heads, self.config.head_dim).transpose(1, 2)\n        \n        attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=mask)\n        return self.o_proj((attn_output * g).transpose(1, 2).contiguous().view(bsz, seqlen, -1)), None\n\n\nprint(\"Attention mechanisms defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:48:22.454572Z","iopub.execute_input":"2025-07-11T06:48:22.455233Z","iopub.status.idle":"2025-07-11T06:48:22.471632Z","shell.execute_reply.started":"2025-07-11T06:48:22.455206Z","shell.execute_reply":"2025-07-11T06:48:22.470748Z"}},"outputs":[{"name":"stdout","text":"Attention mechanisms defined\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Minimax MOE Layer","metadata":{}},{"cell_type":"code","source":"def load_balancing_loss_func(logits, num_experts):\n    if not logits: return 0.0\n    probs = F.softmax(torch.cat([l.view(-1, num_experts) for l in logits], 0), -1)\n    p_mean = probs.mean(0)\n    return torch.sum(p_mean * p_mean) * num_experts\n\n\nclass Expert(nn.Module):\n    def __init__(self, config): \n        super().__init__()\n        self.w_in = nn.Linear(config.hidden_size, config.ffn_hidden_dim, False)\n        self.w_out = nn.Linear(config.ffn_hidden_dim, config.hidden_size, False)\n        self.act_fn = nn.SiLU()\n        \n    def forward(self, x): \n        return self.w_out(self.act_fn(self.w_in(x)))\n        \n\nclass MoE(nn.Module):\n    def __init__(self, config):\n        super().__init__(); \n        self.gate =  nn.Linear(config.hidden_size, config.num_experts, False)\n        self.experts = nn.ModuleList([Expert(config) for _ in range(config.num_experts)])\n        self.k = config.num_experts_per_tok\n        \n    def forward(self, x):\n        bsz, seqlen, dim = x.shape; \n        x_flat = x.view(-1, dim)\n        router_logits = self.gate(x_flat)\n        routing_weights, selected_experts = torch.topk(F.softmax(router_logits, 1, dtype=torch.float), self.k, -1)\n        routing_weights = (routing_weights / routing_weights.sum(-1, True)).to(x.dtype)\n        final_output = torch.zeros_like(x_flat)\n        \n        for i in range(self.k):\n            for j in range(len(self.experts)):\n                token_mask = selected_experts[:, i] == j\n                if token_mask.any(): final_output[token_mask] += self.experts[j](x_flat[token_mask]) * routing_weights[token_mask, i].unsqueeze(-1)\n        return final_output.view(bsz, seqlen, dim), router_logits.view(bsz, seqlen, -1)\n\n\nprint(\"MoE components defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:49:53.811162Z","iopub.execute_input":"2025-07-11T06:49:53.811908Z","iopub.status.idle":"2025-07-11T06:49:53.825394Z","shell.execute_reply.started":"2025-07-11T06:49:53.811875Z","shell.execute_reply":"2025-07-11T06:49:53.824441Z"}},"outputs":[{"name":"stdout","text":"MoE components defined\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Final Minimax Blocks","metadata":{}},{"cell_type":"code","source":"class MiniMaxBlock(nn.Module):\n    def __init__(self, config, is_softmax):\n        super().__init__()\n        self.attention = SoftmaxAttention(config) if is_softmax else LightningAttention(config)\n        self.moe_ffn = MoE(config)\n        self.attention_norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n        self.ffn_norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n        self.deepnorm_alpha = config.deepnorm_alpha\n        \n    def forward(self, x, mask, output_attentions=False):\n        attn_out, attn_weights = self.attention(self.attention_norm(x), mask, output_attentions)\n        h = x + self.deepnorm_alpha * attn_out\n        ffn_out, router_logits = self.moe_ffn(self.ffn_norm(h))\n        return h + self.deepnorm_alpha * ffn_out, router_logits, attn_weights\n\n\n\nclass MiniMaxText01ForCausalLM(nn.Module):\n    def __init__(self, config: MiniMaxConfig):\n        super().__init__()\n        self.config = config\n        self.tok_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.layers = nn.ModuleList([MiniMaxBlock(config, (i + 1) % config.softmax_attention_period == 0) for i in range(config.num_layers)])\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        self.tok_embeddings.weight = self.lm_head.weight\n\n    def forward(self, input_ids, labels=None, output_attentions=False, **kwargs):\n        bsz, seqlen = input_ids.shape\n        h = self.tok_embeddings(input_ids)\n        mask = torch.triu(torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=input_ids.device), 1).type_as(h)\n        all_router_logits, all_attns = [], []\n        \n        for layer in self.layers:\n            h, logits, attns = layer(h, mask, output_attentions)\n            all_router_logits.append(logits)\n            if output_attentions:\n                all_attns.append(attns)\n        \n        logits = self.lm_head(self.norm(h))\n        loss = None\n        \n        if labels is not None:\n            loss = F.cross_entropy(logits[..., :-1, :].reshape(-1, self.config.vocab_size), labels[..., 1:].reshape(-1))\n            loss += self.config.router_aux_loss_coef * load_balancing_loss_func(all_router_logits, self.config.num_experts)\n        \n        return {\"loss\": loss, \"logits\": logits, \"attentions\": all_attns if output_attentions else None}\n        \n\n    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n        return {\"input_ids\": input_ids}\n        \n\n    @torch.no_grad()\n    def generate(self, input_ids: torch.Tensor, max_length: int, eos_token_id: int, temperature: float = 1.0, top_k: int = 50):\n        self.eval()\n        for _ in range(max_length - input_ids.shape[1]):\n            model_inputs = self.prepare_inputs_for_generation(input_ids)\n            \n            outputs = self.forward(**model_inputs)\n            logits = outputs['logits']\n            \n            next_token_logits = logits[:, -1, :]\n            \n            if temperature != 1.0:\n                next_token_logits = next_token_logits / temperature\n\n            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n            \n            top_k_probs = F.softmax(top_k_logits, dim=-1)\n            next_token_index = torch.multinomial(top_k_probs, num_samples=1)\n            next_token = torch.gather(top_k_indices, -1, next_token_index)\n\n            input_ids = torch.cat([input_ids, next_token], dim=-1)\n\n            if next_token.item() == eos_token_id:\n                break\n                \n        return input_ids\n\n\nprint(\"MiniMax-01 model defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:48:31.435795Z","iopub.execute_input":"2025-07-11T06:48:31.436066Z","iopub.status.idle":"2025-07-11T06:48:31.448931Z","shell.execute_reply.started":"2025-07-11T06:48:31.436048Z","shell.execute_reply":"2025-07-11T06:48:31.448053Z"}},"outputs":[{"name":"stdout","text":"MiniMax-01 model defined\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## General Utility Functions","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(epoch, step, model, optimizer, scheduler, loss_history, path):\n    state = {'epoch': epoch, \n             'step': step, \n             'model_state_dict': model.state_dict(),\n             'optimizer_state_dict': optimizer.state_dict(), \n             'scheduler_state_dict': scheduler.state_dict(),\n             'loss_history': loss_history}\n    \n    torch.save(state, os.path.join(path, \"checkpoint.pth\"))\n    print(f\"Checkpoint saved at Epoch {epoch+1}, Step {step+1}\")\n    \n\n\ndef load_checkpoint(model, optimizer, scheduler, path):\n    start_epoch, start_step, loss_history = 0, 0, []\n    ckpt_path = os.path.join(path, \"checkpoint.pth\")\n    \n    if os.path.exists(ckpt_path):\n        ckpt = torch.load(ckpt_path)\n        model.load_state_dict(ckpt['model_state_dict'])\n        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n        scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n        start_epoch, start_step, loss_history = ckpt['epoch'], ckpt['step'] + 1, ckpt['loss_history']\n        \n        print(f\"Checkpoint found. Resuming training from Epoch {start_epoch+1}, Step {start_step}.\")\n        \n    else:\n        print(\"No checkpoint found. Starting training from scratch.\")\n        \n    return start_epoch, start_step, loss_history\n\n\n\ndef plot_loss_curve(loss_history, save_path):\n    plt.figure(figsize=(12, 6))\n    plt.plot(loss_history, label='Training Loss')\n    \n    plt.title('Training Loss Curve')\n    plt.xlabel('Steps')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.grid(True)\n    plt.savefig(os.path.join(save_path, \"loss_curve.png\"))\n    plt.show()\n    \n    print(f\"Loss curve saved to {save_path}\")\n    \n\nprint(\"Helper functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:48:38.516311Z","iopub.execute_input":"2025-07-11T06:48:38.516922Z","iopub.status.idle":"2025-07-11T06:48:38.528342Z","shell.execute_reply.started":"2025-07-11T06:48:38.516891Z","shell.execute_reply":"2025-07-11T06:48:38.527421Z"}},"outputs":[{"name":"stdout","text":"Helper functions defined\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Training ","metadata":{}},{"cell_type":"code","source":"def train_llm():\n    BATCH_SIZE = 16\n    NUM_EPOCHS = 3\n    LEARNING_RATE = 3e-5\n    \n    PRINT_INTERVAL, CHECKPOINT_INTERVAL = 50, 200\n\n    \n    print(f\"--- Loading pre-processed data from '{DATA_PATH}' ---\")\n    try:\n        train_ds = load_from_disk(os.path.join(DATA_PATH, \"train\"))\n        eval_ds = load_from_disk(os.path.join(DATA_PATH, \"validation\"))\n        tokenizer = AutoTokenizer.from_pretrained(DATA_PATH)\n        print(\"Data loaded successfully.\")\n        \n    except FileNotFoundError:\n        print(f\"Error: Processed data not found. Please run the Data Preparation cell first.\")\n        return\n\n    \n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n    train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator)\n    eval_dataloader = DataLoader(eval_ds, batch_size=BATCH_SIZE, collate_fn=data_collator)\n\n    \n    print(f\"\\nInitializing model on {DEVICE}...\")\n    config = MiniMaxConfig(len(tokenizer))\n    model_no_dp = MiniMaxText01ForCausalLM(config)\n\n    \n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs via DataParallel!\")\n        model = nn.DataParallel(model_no_dp)\n    else:\n        model = model_no_dp\n\n    \n    model.to(DEVICE)\n    model_size = sum(p.numel() for p in model_no_dp.parameters()) / 1e6\n    print(f\"Model Initialized. Total parameters: {model_size:.2f}M\")\n\n    \n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n    num_training_steps = NUM_EPOCHS * len(train_dataloader)\n    lr_scheduler = get_scheduler(\"linear\", optimizer, 0, num_training_steps)\n\n    start_epoch, resume_step, loss_history = load_checkpoint(model_no_dp, optimizer, lr_scheduler, INPUT_CHECKPOINT_PATH)\n\n    \n    print(f\"\\n--- Starting Training ---\")\n    model.train()\n\n    \n    for epoch in range(start_epoch, NUM_EPOCHS):\n        print(f\"\\n--- Starting Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n        if resume_step >= len(train_dataloader):\n            print(\"Already completed this epoch. Skipping.\") \n            resume_step = 0\n            continue\n        if resume_step > 0: \n            print(f\"Resuming from step {resume_step}...\")\n\n            \n        data_iterator = iter(train_dataloader)\n        for _ in range(resume_step):\n             try:\n                next(data_iterator)\n             except StopIteration:\n                break\n\n        \n        for step, batch in enumerate(data_iterator, start=resume_step):\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            \n            _, loss_outputs, _ = model(**batch)\n            loss = loss_outputs.mean()\n            \n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            \n            loss_history.append(loss.item())\n            \n            if (step + 1) % PRINT_INTERVAL == 0:\n                print(f\"Epoch {epoch+1}, Step {step+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n            \n            if (step + 1) % CHECKPOINT_INTERVAL == 0:\n                save_checkpoint(epoch, step, model_no_dp, optimizer, lr_scheduler, loss_history, CHECKPOINT_PATH)\n        \n        resume_step = 0\n\n    \n    print(\"\\n ---Training Complete ---\")\n    save_checkpoint(NUM_EPOCHS - 1, len(train_dataloader) - 1, model_no_dp, optimizer, lr_scheduler, loss_history, CHECKPOINT_PATH)\n    plot_loss_curve(loss_history, RESULTS_PATH)\n    \n    print(\"\\n --- Starting Final Evaluation for Perplexity ---\")\n    model.eval()\n    perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n\n    \n    for batch in eval_dataloader:\n        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n        with torch.no_grad():\n            logits, _, _ = model(**batch)\n        perplexity_metric.add_batch(predictions=logits, references=batch[\"input_ids\"])\n    \n    results = perplexity_metric.compute(model_id='minimax_replica_kaggle')\n\n    \n    print(f\"\\n--- Final Analysis & Results ---\")\n    model_size = sum(p.numel() for p in model_no_dp.parameters()) / 1e6\n    print(f\"Model Size: {model_size:.2f}M parameters\")\n    print(f\"Final Validation Perplexity: {results['mean_perplexity']:.2f} (Lower is better)\")\n    \n    print(\"\\n--- Model trained ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train_llm()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def generate_base_completion(prompt, model, tokenizer, max_length=100):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n\n    print(f\"\\n--- Prompt: ---\\n'{prompt}'\")\n    print(\"\\n--- Generating Completion... ---\")\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=inputs['input_ids'],\n            max_length=max_length,\n            eos_token_id=tokenizer.eos_token_id,\n            temperature=0.7,\n            top_k=50\n        )\n        \n    completion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    print(f\"\\n--- Base Model Completion: ---\\n{completion}\")\n    print(\"-\" * 40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:48:45.286532Z","iopub.execute_input":"2025-07-11T06:48:45.286812Z","iopub.status.idle":"2025-07-11T06:48:45.291780Z","shell.execute_reply.started":"2025-07-11T06:48:45.286793Z","shell.execute_reply":"2025-07-11T06:48:45.290953Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer\n\n\nprint(\"--- Loading Tokenizer and Base Model ---\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\nexcept Exception:\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n\nconfig = MiniMaxConfig(vocab_size=len(tokenizer))\nmodel = MiniMaxText01ForCausalLM(config) \n\n\ncheckpoint_file = os.path.join(BASE_MODEL_INPUT_PATH, \"checkpoint.pth\")\nif os.path.exists(checkpoint_file):\n    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(\"✅ Base model weights loaded successfully.\")\nelse:\n    raise FileNotFoundError(f\"Base model checkpoint not found at '{checkpoint_file}'.\")\n\n\nmodel.to(DEVICE)\nmodel.eval()\n\n\nprompt1 = \"The history of the Roman Empire is a fascinating subject, particularly the transition from the Republic to the Principate, which began\"\nprompt2 = \"Artificial intelligence is a field of computer science that focuses on creating systems capable of\"\nprompt3 = \"The poem began with the line, 'Once upon a time, in a land filled with towering mountains and'\"\n\ngenerate_base_completion(prompt1, model, tokenizer)\ngenerate_base_completion(prompt2, model, tokenizer)\ngenerate_base_completion(prompt3, model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:50:01.823094Z","iopub.execute_input":"2025-07-11T06:50:01.823788Z","iopub.status.idle":"2025-07-11T06:50:21.797465Z","shell.execute_reply.started":"2025-07-11T06:50:01.823754Z","shell.execute_reply":"2025-07-11T06:50:21.796563Z"}},"outputs":[{"name":"stdout","text":"--- Loading Tokenizer and Base Model ---\n✅ Base model weights loaded successfully.\n\n--- Prompt: ---\n'The history of the Roman Empire is a fascinating subject, particularly the transition from the Republic to the Principate, which began'\n\n--- Generating Completion... ---\n\n--- Base Model Completion: ---\nThe history of the Roman Empire is a fascinating subject, particularly the transition from the Republic to the Principate, which began with the Roman @-@ Roman War ( AD 8 – 14 ) . \n = = = Early history = = = \n The Roman Empire 's history has a profound influence on the history of the Roman Empire . In the beginning of the Roman Empire , the empire was controlled by the Roman Empire . Roman society has been called the Roman Empire . A Roman Empire developed\n----------------------------------------\n\n--- Prompt: ---\n'Artificial intelligence is a field of computer science that focuses on creating systems capable of'\n\n--- Generating Completion... ---\n\n--- Base Model Completion: ---\nArtificial intelligence is a field of computer science that focuses on creating systems capable of investigating a variety of actions . \n = = History = = \n The first published work of the journal <unk> was in the journal Proceedings of the American Philosophical Society . There were extensive research of the book , including illustrations and illustrations from various authors , as well as a number of other written books on the subject and the publication of a detailed book of the subject . The book was published in a supplement to\n----------------------------------------\n\n--- Prompt: ---\n'The poem began with the line, 'Once upon a time, in a land filled with towering mountains and''\n\n--- Generating Completion... ---\n\n--- Base Model Completion: ---\nThe poem began with the line, 'Once upon a time, in a land filled with towering mountains and'roll land . \" \n = = = = = \" Last night <unk> and the <unk> of the Great Debate \" = = = = = \n After the first \" Last night <unk> \" , the poem was published in the United States on July 2 , 1867 . It has been called \" the most famous poem in English \" . Its first tenor was\n----------------------------------------\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Instruct-Train (SFT)","metadata":{}},{"cell_type":"markdown","source":"## Base Model definition - must run","metadata":{}},{"cell_type":"code","source":"print(\"--- Loading Tokenizer ---\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/minimax-processed-data/processed_data/\")\nexcept Exception:\n    print(\"Pre-saved tokenizer not found. Loading 'gpt2' as a fallback.\")\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n\nprint(\"\\n--- Loading Pre-trained Base Model ---\")\nconfig = MiniMaxConfig(vocab_size=len(tokenizer))\nbase_model = MiniMaxText01ForCausalLM(config)\n\n\ncheckpoint_file = os.path.join(BASE_MODEL_INPUT_PATH, \"checkpoint.pth\")\nif os.path.exists(checkpoint_file):\n    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n    base_model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\" Pre-trained base model weights loaded successfully \")\nelse:\n    raise FileNotFoundError(f\"Base model checkpoint not found at '{checkpoint_file}'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:58:23.859242Z","iopub.execute_input":"2025-07-11T06:58:23.860027Z","iopub.status.idle":"2025-07-11T06:58:26.783135Z","shell.execute_reply.started":"2025-07-11T06:58:23.860004Z","shell.execute_reply":"2025-07-11T06:58:26.782448Z"}},"outputs":[{"name":"stdout","text":"--- Loading Tokenizer ---\nPre-saved tokenizer not found. Loading 'gpt2' as a fallback.\n\n--- Loading Pre-trained Base Model ---\n Pre-trained base model weights loaded successfully \n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## LoRA Configuration","metadata":{}},{"cell_type":"code","source":"for param in base_model.parameters():\n  param.requires_grad = False\n\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n)\n\n\nprint(\"\\nApplying LoRA adapters to the model...\")\nsft_model = get_peft_model(base_model, lora_config)\nsft_model = sft_model.to(DEVICE)\n\n\nprint(\"\\n--- LoRA Model Summary ---\")\nsft_model.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Pre-processing","metadata":{}},{"cell_type":"code","source":"def format_prompt(sample):\n    prompt = \"\"\n    for message in sample['messages']:\n        if message['role'] == 'user':\n            prompt += f\"### HUMAN:\\n{message['content']}\\n\\n\"\n        else:\n            prompt += f\"### ASSISTANT:\\n{message['content']}\\n\\n\"\n    return prompt\n\n\ndef prepare_sft_dataset(tokenizer):\n    print(f\"\\n--- Preparing SFT dataset: ultrachat_200k ---\")\n    dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n    dataset = dataset.select(range(200000))\n    original_columns = dataset.column_names\n\n\n    def tokenize_and_format(element):\n        formatted_prompt = format_prompt(element)\n        \n        outputs = tokenizer(\n            formatted_prompt,\n            truncation=True,\n            padding=\"max_length\", \n            max_length=512,\n            add_special_tokens=False,\n        )\n        \n        outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n        return outputs\n\n    print(\"Tokenizing and formatting dataset...\")\n    tokenized_dataset = dataset.map(\n        tokenize_and_format,\n        batched=False, \n        remove_columns=original_columns,\n    )\n\n    \n    tokenized_dataset.set_format(\n        type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n    )\n    print(f\"SFT dataset prepared. Using {len(tokenized_dataset)} samples.\")\n    return tokenized_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sft_dataset = prepare_sft_dataset(tokenizer)\nsft_dataloader = DataLoader(\n    sft_dataset,\n    batch_size=4,\n    shuffle=True\n)\n\nprint(\"DataLoaders for SFT defined\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"\nNUM_SFT_EPOCHS = 1\nSFT_LEARNING_RATE = 2e-4\nSFT_CHECKPOINT_INTERVAL = 500\n\n\noptimizer = AdamW(filter(lambda p: p.requires_grad, sft_model.parameters()), lr=SFT_LEARNING_RATE)\nnum_training_steps = NUM_SFT_EPOCHS * len(sft_dataloader)\nlr_scheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n\nprint(\"\\n--- Starting Supervised Fine-Tuning with LoRA ---\")\nsft_model.train()\n\nloss_history = []\n\nfor epoch in range(NUM_SFT_EPOCHS):\n    progress_bar = tqdm(sft_dataloader, desc=f\"SFT Epoch {epoch+1}\")\n    \n    for step, batch in enumerate(progress_bar):\n        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n        \n        outputs = sft_model(**batch)\n        loss = outputs['loss']\n        \n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        \n        loss_history.append(loss.item())\n        \n        progress_bar.set_postfix({\"loss\": loss.item()})\n\n        if (step + 1) % SFT_CHECKPOINT_INTERVAL == 0:\n            checkpoint_save_path = os.path.join(SFT_LORA_CHECKPOINT_PATH, f\"step_{step+1}\")\n            print(f\"\\nSaving LoRA adapter checkpoint to {checkpoint_save_path}...\")\n            sft_model.save_pretrained(checkpoint_save_path)\n\n\nfinal_save_path = os.path.join(SFT_LORA_CHECKPOINT_PATH, \"final\")\nprint(f\"\\n--- SFT Complete. Saving final LoRA adapter weights to {final_save_path} ---\")\nsft_model.save_pretrained(final_save_path)\n\n\nprint(\"\\n--- Generating SFT Loss Curve ---\")\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nplt.plot(loss_history, label='SFT Training Loss')\nplt.title('SFT LoRA Fine-Tuning Loss Curve')\n\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\nplt.savefig(os.path.join(KAGGLE_WORKING_PATH, \"sft_loss_curve.png\"))\nplt.show()\n\nprint(\"SFT workflow finished and loss curve saved.\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def generate_response(prompt, model, tokenizer, device, max_length=150):\n    formatted_prompt = f\"### HUMAN:\\n{prompt}\\n\\n### ASSISTANT:\\n\"\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n    print(f\"\\n--- Prompt: ---\\n{prompt}\")\n    print(\"\\n--- Generating Response... ---\")\n    \n    outputs = model.generate(\n        input_ids=inputs['input_ids'],\n        max_length=max_length,\n        eos_token_id=tokenizer.eos_token_id,\n        temperature=0.7,\n        top_k=50\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    assistant_response = response.split(\"### ASSISTANT:\\n\")[-1]\n    print(f\"\\n--- Model Response: ---\\n{assistant_response}\")\n    print(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:50:42.555282Z","iopub.execute_input":"2025-07-11T06:50:42.555581Z","iopub.status.idle":"2025-07-11T06:50:42.561435Z","shell.execute_reply.started":"2025-07-11T06:50:42.555560Z","shell.execute_reply":"2025-07-11T06:50:42.560739Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"LORA_ADAPTER_PATH = \"/kaggle/input/beens-lora/pytorch/default/1/lora_adapter/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:50:48.488489Z","iopub.execute_input":"2025-07-11T06:50:48.489343Z","iopub.status.idle":"2025-07-11T06:50:48.493732Z","shell.execute_reply.started":"2025-07-11T06:50:48.489314Z","shell.execute_reply":"2025-07-11T06:50:48.492977Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print(f\"\\n--- Loading LoRA adapters from: {LORA_ADAPTER_PATH} ---\")\nmodel = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)\nmodel = model.merge_and_unload()\nmodel.to(DEVICE)\nmodel.eval()\nprint(\"LoRA adapters merged into the base model.\")\n\ngenerate_response(\"Hello! What can you do?\", model, tokenizer, DEVICE)\ngenerate_response(\"What is the capital of France?\", model, tokenizer, DEVICE)\ngenerate_response(\"Write a short, three-line poem about a cat.\", model, tokenizer, DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:58:40.015275Z","iopub.execute_input":"2025-07-11T06:58:40.015911Z","iopub.status.idle":"2025-07-11T06:58:54.351756Z","shell.execute_reply.started":"2025-07-11T06:58:40.015883Z","shell.execute_reply":"2025-07-11T06:58:54.351089Z"}},"outputs":[{"name":"stdout","text":"\n--- Loading LoRA adapters from: /kaggle/input/beens-lora/pytorch/default/1/lora_adapter/ ---\nLoRA adapters merged into the base model.\n\n--- Prompt: ---\nHello! What can you do?\n\n--- Generating Response... ---\n\n--- Model Response: ---\n1. You might have the same money as you could have. You may have the same money as you are spending money to you. You may have the same money to have the same money or have the same money to have the same work.\n\n2. You may have the same money or may have the same money or may have the same money or may have the same money or can have the same money or is often used. You may have the same money or may have the same money or may have the same money or may have the same money or may have a specific role in your contract.\n\n3. You may have the\n------------------------------\n\n--- Prompt: ---\nWhat is the capital of France?\n\n--- Generating Response... ---\n\n--- Model Response: ---\nThe economic importance of the country is in the northern hemisphere and has the economic importance of the country. The most significant economic impact of the country is in the eastern\n------------------------------\n\n--- Prompt: ---\nWrite a short, three-line poem about a cat.\n\n--- Generating Response... ---\n\n--- Model Response: ---\nI am a cat.\n\nI am an elephant, I feel like a giant.\n\nI am a lion, I will never forget it.\n\nI am a lion, I will hear my eyes.\n\nI am a lion, I will hear my eyes.\n\nI am a lion, I love to be a hero.\n\nI am a lion, I have the ability to sing and sing, and I will hear my eyes in your eyes.\n\nI am a lion, I can hear my eyes at a moment.\n\nI am a lion, I will hear my eyes\n------------------------------\n","output_type":"stream"}],"execution_count":22}]}